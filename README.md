# Generative AI Project

## Overview
This repository contains a **Generative AI** project built using **Google's Gemini Model**. The application allows users to interact with the Gemini large language model (LLM) via a Streamlit interface for generating text and image-based content. The project consists of three main applications: a text generation demo, an image generation demo, and a chat-based question-answering demo, showcasing the capabilities of the Gemini LLM.

## Features

### Text Generation Application (`app.py`)
- Users can input a question or text prompt to receive a generated response from the Gemini LLM (Gemini Pro).
- Streamlit provides a simple interface to capture the user input and display AI-generated responses in real time.

### Image Generation Application (`vision.py`)
- This application allows users to upload an image and provide an optional text prompt.
- The Gemini LLM (Gemini 1.5 Flash) is used to generate content based on both the image and the text input.
- Supports `.jpg`, `.jpeg`, and `.png` image formats, with the output displayed directly in the Streamlit interface.

### Chat-Based Q&A Application (`qachat.py`)
- A chat-based interaction system where users can ask questions, and the Gemini LLM provides real-time responses.
- The chat history is maintained using Streamlit's session state, allowing for multi-turn conversation.
- Users can see both their inputs and the AI's responses in a conversational format.
- This feature uses the Gemini Pro model for generating responses, supporting streaming of response chunks for a more interactive experience.

## Project Structure

- `app.py`: Contains the implementation of the text generation application using the Gemini LLM. It uses Streamlit to capture text input from the user and display AI-generated responses.
- `vision.py`: Implements the image generation demo where users can upload an image and provide a prompt for the Gemini LLM to generate content based on the image.
- `qachat.py`: Implements the chat-based question-answering system where users can interact with the Gemini LLM in a conversational manner, with session history preserved.
- `requirements.txt`: Specifies the dependencies required to run the project, including Streamlit, Google's Generative AI library, and dotenv for environment variable management.

## Installation

To run this project locally, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/Rushikeshkadam1/Genarative-AI-project.git
   cd Genarative-AI-project

2. Create a virtual environment and activate it (optional but recommended):
   ```bash
   python3 -m venv venv
   source venv/bin/activate

3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt

4. Set up environment variables:
   ```bash
   GOOGLE_API_KEY=your_google_api_key

5. Run the applications:
    ```bash
    For the text generation demo:
    streamlit run app.py

    For the image generation demo:
    streamlit run vision.py

    For the chat-based Q&A demo:
    streamlit run qachat.py

## Dependencies

The project relies on the following libraries (specified in requirements.txt):

streamlit: A web app framework for creating interactive UIs.

google-generativeai: Google's library for accessing Generative AI models like Gemini.

python-dotenv: A library for loading environment variables from a .env file.


## Usage

## Text Generation (app.py)
1. Run the app:
     ```bash
     streamlit run app.py
2. Enter a question or prompt in the input box.
3. Click the "Ask the question" button.
4. The response from the Gemini LLM will be displayed on the screen.

##Image Generation (vision.py)
1. Run the app:
   ```bash
    streamlit run vision.py
2. Upload an image (JPEG or PNG formats) using the file uploader.
3. Optionally, provide a text prompt for context.
4. Click the "Get response" button to see the output generated by the Gemini model.

## Chat-Based Q&A Demo (qachat.py)
1. Run the app:
   ```bash
   streamlit run qachat.py
2. Enter a question in the input box.
3. Click the "Ask any question" button.
4. The response from the Gemini LLM will appear in real time.
5. Chat history will be displayed, showing both the user's questions and the AI's responses for multi-turn interaction.



## Acknowledgments
This project uses Google's Generative AI Gemini models for generating content.
Streamlit is used as the front-end for quick prototyping of AI applications.
     
